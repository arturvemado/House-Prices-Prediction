{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# library imports\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from scipy import stats\n",
    "from scipy.stats import norm, skew\n",
    "import tqdm\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set the path of the raw data\n",
    "raw_data_path = os.path.join(os.path.pardir, 'data', 'raw')\n",
    "train_file_path = os.path.join(raw_data_path, 'train_house.csv')\n",
    "test_file_path = os.path.join(raw_data_path, 'test_house.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read the data with all default parameters\n",
    "train_df = pd.read_csv(train_file_path, index_col='Id')\n",
    "test_df = pd.read_csv(test_file_path, index_col='Id')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Organizing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get columns with nan values from training data\n",
    "# numerical columns and categorical columns in the training data\n",
    "col_with_nan = [col for col in train_df.columns if train_df[col].isnull().any()]\n",
    "#num_col = [col for col in train_df.columns if train_df[col].dtypes in ['float64', 'int64']]\n",
    "#cat_col = list(set(train_df.columns) - set(num_col))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Basic Structure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check for duplicates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Try to find duplicated data\n",
    "duplicatedID = train_df.duplicated()\n",
    "train_df[duplicatedID] #no duplicated found"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Study the data graphically"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hist_log_hist (df, col, show=True, file_path=''):\n",
    "    \n",
    "    '''\n",
    "        Returns the histogram and log1p of the values histogram of the column col\n",
    "        with their probability plots.\n",
    "        \n",
    "        Args:\n",
    "            df: Dataframe with features. Shape(m, n)\n",
    "            col: feature to be plotted\n",
    "            show: display graphics if True\n",
    "\n",
    "    '''\n",
    "\n",
    "    f, axes = plt.subplots(2, 2, figsize=(12, 10));\n",
    "\n",
    "    # original histogram\n",
    "    sns.distplot(df[df[col].notnull()][col], fit=norm, ax=axes[0, 0]);\n",
    "    stats.probplot(df[df[col].notnull()][col], plot=axes[1, 0]);\n",
    "\n",
    "    # log-histogram (deals with zero and NaN values)\n",
    "    sns.distplot(np.log1p(df[df[col].notnull()][col]), fit=norm, ax=axes[0, 1]);\n",
    "    stats.probplot(np.log1p(df[df[col].notnull()][col]), plot=axes[1, 1]);\n",
    "    \n",
    "    plt.suptitle(col, fontsize=18)\n",
    "    plt.tight_layout(rect=[0, 0, 1, 0.95]);\n",
    "    \n",
    "    if show:\n",
    "        plt.show();\n",
    "    elif file_path:\n",
    "        plt.savefig(file_path, dpi=300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# distributions of numerical features\n",
    "# saving histograms for all numerical columns to see skewness of data\n",
    "\n",
    "for col in tqdm.tqdm(num_col):\n",
    "    \n",
    "    path = os.path.join(os.path.pardir, 'data', 'figures', col)\n",
    "    path = '.'.join([path, 'png'])\n",
    "    hist_log_hist(train_df, col, show=False, file_path=path)\n",
    "    plt.cla()\n",
    "    plt.clf()\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# very skewed columns without 'SalePrice'\n",
    "# skew_col = ['1stFlrSF', 'GrLivArea', 'LotArea']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check scatter plot with the target \"SalePrice\"\n",
    "for col in tqdm.tqdm(num_col):\n",
    "    \n",
    "    path = os.path.join(os.path.pardir, 'data', 'figures', col)\n",
    "    path = '.'.join([path, 'png'])\n",
    "    plt.scatter(train_df[col], train_df['SalePrice'])\n",
    "    plt.savefig(path, dpi=300)\n",
    "    plt.cla()\n",
    "    plt.clf()\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Removing outiliers shown in the scatter plot. Points that did not follow the patterns in the scatter plot were dropped"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# outliers from scatter plot\n",
    "# point > 4000 in 1stFlrSF\n",
    "# point > 5000 in BsmtFinSF1\n",
    "# point >~ 4500 in GrLivArea\n",
    "# point > 100000 in LotArea\n",
    "# point > 300 in LotFrontage\n",
    "# drop outliers from above\n",
    "\n",
    "outliers_removed = train_df.copy()\n",
    "outliers_removed = outliers_removed.drop(outliers_removed[outliers_removed['1stFlrSF'] >= 4000].index)\n",
    "outliers_removed = outliers_removed.drop(outliers_removed[outliers_removed.BsmtFinSF1 >= 5000].index)\n",
    "outliers_removed = outliers_removed.drop(outliers_removed[outliers_removed.GrLivArea > 4500].index)\n",
    "outliers_removed = outliers_removed.drop(outliers_removed[outliers_removed.LotArea > 100000].index)\n",
    "outliers_removed = outliers_removed.drop(outliers_removed[outliers_removed.LotFrontage > 300].index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check how each categorical feature influence the target sale price for the houses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check importance of categorical features\n",
    "for col in tqdm.tqdm(cat_col):\n",
    "    \n",
    "    path = os.path.join(os.path.pardir, 'data', 'figures', col)\n",
    "    path = '.'.join([path, 'png'])\n",
    "    \n",
    "    # order bars by its mean\n",
    "    order = outliers_removed.groupby([col])['SalePrice'].aggregate(\n",
    "        np.mean).reset_index().sort_values('SalePrice')\n",
    "    \n",
    "    plt.figure(figsize=(12, 7))\n",
    "    \n",
    "    sns.barplot(x=col, y='SalePrice', data=outliers_removed, order=order[col]);\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig(path, dpi=300)\n",
    "    plt.cla()\n",
    "    plt.clf()\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Keeping track of the freatures that showed some ranking in the categories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # columns with clear rankings\n",
    "# rank_col = ['Alley', 'BsmtCond', 'BsmtExposure', 'BsmtQual',\n",
    "#             'CentralAir', 'Electrical', 'ExterCond', 'Condition2', \n",
    "#             'Exterior1st', 'Exterior2nd', 'ExterQual', 'Fence', \n",
    "#             'FireplaceQU', 'Foundation', 'Functional', 'GarageFinish', \n",
    "#             'GarageQual', 'GarageType', 'Heating', 'HeatingQC', 'HouseStyle',\n",
    "#             'KitchenQual', 'LandContour', 'MasVnrType', 'MSZoning', 'Neighborhood', \n",
    "#             'PavedDrive', 'PoolQC', 'RoofMatl', 'RoofStyle', 'SaleType', 'Street',\n",
    "#             'Utilities']\n",
    "# # maybe target encoder columns (not so clear ranking) or one hot encoder\n",
    "# te_col = ['BldgType', 'BsmtFinType1', 'BsmtFinType2', 'Condition1', 'GarageCond', \n",
    "#           'LandSlope', 'LotConfig', 'LotShape', 'MiscFeature', 'SaleCondition']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Checking numerical features that are actually categorical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_cat_col = ['MSSubClass', 'OverallQual', 'OverallCond']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for col in tqdm.tqdm(num_cat_col):\n",
    "    \n",
    "    path = os.path.join(os.path.pardir, 'data', 'figures', col)\n",
    "    path = '.'.join([path, 'png'])\n",
    "    \n",
    "    # order bars by its mean\n",
    "    order = outliers_removed.groupby([col])['SalePrice'].aggregate(\n",
    "        np.mean).reset_index().sort_values('SalePrice')\n",
    "    \n",
    "    plt.figure(figsize=(12, 7))\n",
    "    \n",
    "    sns.barplot(x=col, y='SalePrice', data=outliers_removed, order=order[col]);\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig(path, dpi=300)\n",
    "    plt.cla()\n",
    "    plt.clf()\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the figures we can see that MSSubClass is not actually in order, so we will pass it to categorical feature."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MSSubClass\n",
    "train_df['MSSubClass'] = train_df['MSSubClass'].astype('object')\n",
    "test_df['MSSubClass'] = test_df['MSSubClass'].astype('object')\n",
    "\n",
    "# Year and Month sold are transformed into categorical features.\n",
    "train_df['YrSold'] = train_df['YrSold'].astype('object')\n",
    "test_df['YrSold'] = test_df['YrSold'].astype('object')\n",
    "train_df['MoSold'] = train_df['MoSold'].astype('object')\n",
    "test_df['MoSold'] = test_df['MoSold'].astype('object')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Missing values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = outliers_removed.copy()\n",
    "train_df[col_with_nan].info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# missing values dataframe to avoid errors in code\n",
    "missing = train_df.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is probable that houses have similar LotFrontage in the same Neighborhood"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fill missing values of lotfrontage with values of the median in the neighborhood\n",
    "missing['LotFrontage'] = missing.groupby(['Neighborhood'])['LotFrontage'].transform(\n",
    "    lambda x: x.fillna(x.median()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All these features are probably not in the houses with missing values, so we will impute 'NA'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# filling NaN values with 'NA'\n",
    "fill_NA_col = ['BsmtQual', 'BsmtCond', 'BsmtExposure', 'BsmtFinType1','BsmtFinType2',\n",
    "              'GarageType', 'GarageFinish', 'GarageQual', 'GarageCond', 'Alley',\n",
    "              'FireplaceQu', 'PoolQC', 'Fence', 'MiscFeature']\n",
    "\n",
    "for col in fill_NA_col:\n",
    "    missing[col].fillna('NA', inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For MasVnrArea and Type and Electrical we will impute the most frequent values because there are few missing values. For the the GarageYrBlt we will impute zero, because there is no garage in the house."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fill with the most frequent value\n",
    "mode_MasVnr = missing[['MasVnrArea','MasVnrType']].mode().iloc[0,:]\n",
    "missing.MasVnrArea.fillna(mode_MasVnr[0], inplace=True)\n",
    "missing.MasVnrType.fillna(mode_MasVnr[1], inplace=True)\n",
    "\n",
    "# fill the one mv with the most frequent value in the column\n",
    "mode_Electrical = missing.Electrical.mode()[0]\n",
    "missing.Electrical.fillna(mode_Electrical, inplace=True)\n",
    "\n",
    "# filling with year with zero because the garage was not built\n",
    "missing['GarageYrBlt'].fillna(0, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = missing.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Missing Values for test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get cols with NaN values\n",
    "test_col_with_nan = [col for col in test_df.columns if test_df[col].isnull().any()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# protecting original data\n",
    "test_missing = test_df.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_missing[test_col_with_nan].info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get columns with categorical features and numerical ones. We will use it to fill missing values in the test data separately."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# getting numerical and categorical cols in cols with nan\n",
    "cat_col_with_nan = [col for col in test_col_with_nan if test_missing[col].dtypes == 'object']\n",
    "num_col_with_nan = list(set(test_col_with_nan) - set(cat_col_with_nan))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# filling LotFrontage missing values in test data with training median values\n",
    "for neighborhood in list(train_df.groupby(['Neighborhood'])['LotFrontage'].median().index):\n",
    "    \n",
    "    median = train_df[train_df['Neighborhood'] == neighborhood].median()['LotFrontage']\n",
    "    \n",
    "    test_missing.loc[test_missing['Neighborhood'] == neighborhood, 'LotFrontage'] = \\\n",
    "        test_missing.loc[test_missing['Neighborhood'] == neighborhood, 'LotFrontage'].fillna(median)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# replacing missing value in the test data with the most frequent data in train data\n",
    "# for categorical features, and with the median for numerical features\n",
    "for col in cat_col_with_nan:\n",
    "    test_missing[col].fillna(train_df[col].mode()[0], inplace=True)\n",
    "\n",
    "for col in num_col_with_nan:\n",
    "    test_missing[col].fillna(train_df[col].median(), inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# separating 'object', 'int64' and 'float64' columns\n",
    "cat_col = [col for col in train_df.columns if train_df[col].dtypes == 'object']\n",
    "float_col = [col for col in train_df.columns if train_df[col].dtypes == 'float64']\n",
    "int_col = [col for col in train_df.columns if train_df[col].dtypes == 'int64']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fixing dtypes of columns in test data\n",
    "for col in test_missing.columns:\n",
    "    \n",
    "    if col in cat_col:\n",
    "        test_missing = test_missing.astype({col: 'object'})\n",
    "    elif col in int_col:\n",
    "        test_missing = test_missing.astype({col: 'int64'})\n",
    "    elif col in float_col:\n",
    "        test_missing = test_missing.astype({col: 'float64'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df = test_missing.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparing data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Concatenate train and test to manipulate features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = train_df['SalePrice']\n",
    "train_df = train_df.drop('SalePrice', axis=1)\n",
    "all_data = pd.concat([train_df, test_df])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There is only one sample in the training set that has Utilities as 'NoSeWa'. So it will not use this feature."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_data.groupby('Utilities')['Utilities'].count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_data = all_data.drop('Utilities', axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Adding some features that may improve the models predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_data['TotalSF'] = all_data['TotalBsmtSF'] + all_data['1stFlrSF'] + all_data['2ndFlrSF']\n",
    "all_data['TotalPorchArea'] = (all_data['EnclosedPorch'] + all_data['OpenPorchSF'] + \n",
    "                              all_data['3SsnPorch'] + all_data['ScreenPorch'])\n",
    "all_data['HasPool'] = all_data['PoolArea'].apply(lambda x: 1 if x > 0 else 0)\n",
    "all_data['Has2ndFloor'] = all_data['2ndFlrSF'].apply(lambda x: 1 if x > 0 else 0)\n",
    "all_data['HasGarage'] = all_data['GarageArea'].apply(lambda x: 1 if x > 0 else 0)\n",
    "all_data['HasBsmt'] = all_data['TotalBsmtSF'].apply(lambda x: 1 if x > 0 else 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_data['YearBuilt'] = all_data['YearBuilt'].astype('object')\n",
    "all_data['YearRemodAdd'] = all_data['YearRemodAdd'].astype('object')\n",
    "all_data['GarageYrBlt'] = all_data['GarageYrBlt'].astype('object')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Label Encoding ranked features\n",
    "NA_Ex_dict = {'Ex': 5, 'Gd': 4, 'TA': 3, 'Fa': 2, 'Po': 1, 'NA': 0}\n",
    "label_encoding = {'Alley': {'NA': 0, 'Grvl': 1, 'Pave': 2}, \n",
    "                 'ExterQual': NA_Ex_dict,\n",
    "                 'ExterCond': NA_Ex_dict,\n",
    "                 'BsmtQual': NA_Ex_dict,\n",
    "                 'BsmtCond': NA_Ex_dict,\n",
    "                 'HeatingQC': NA_Ex_dict,\n",
    "                 'KitchenQual': NA_Ex_dict,\n",
    "                 'FireplaceQu': NA_Ex_dict,\n",
    "                 'GarageQual': NA_Ex_dict,\n",
    "                 'GarageCond': NA_Ex_dict,\n",
    "                 'PoolQC': NA_Ex_dict,\n",
    "                 'BsmtExposure':{'No': 0, 'Gd': 3, 'Mn': 1, 'Av': 2, 'NA': 0},\n",
    "                 'CentralAir': {'N': 0, 'Y': 1},\n",
    "                 'GarageFinish': {'RFn': 2, 'Unf': 1, 'Fin': 3, 'NA': 0},\n",
    "                 'PavedDrive': {'Y': 2, 'N': 0, 'P': 1},\n",
    "                 'Fence': {'NA': 0, 'MnPrv': 3, 'GdWo': 2, 'GdPrv': 4, 'MnWw': 1},\n",
    "                 'Electrical': {'SBrkr': 5, 'FuseF': 3, 'FuseA': 4, 'FuseP': 2, 'Mix': 1}}\n",
    "\n",
    "all_data.replace(label_encoding, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get remaining object cols\n",
    "object_col = [col for col in all_data.columns if all_data[col].dtypes == 'object']\n",
    "num_col = [col for col in all_data.columns if all_data[col].dtypes != 'object']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "skewed_feats = all_data[num_col].apply(lambda x: skew(x)).sort_values(ascending=False)\n",
    "\n",
    "skewed_feats"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Deal with skewness in the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import skew\n",
    "from scipy.special import boxcox1p\n",
    "from scipy.stats import boxcox_normmax\n",
    "\n",
    "def fixing_skewness(df):\n",
    "    \"\"\"\n",
    "    This function takes in a dataframe and return fixed skewed dataframe\n",
    "    \"\"\"\n",
    "    \n",
    "    ## Getting all the data that are not of \"object\" type. \n",
    "    numeric_feats = df.dtypes[df.dtypes != \"object\"].index\n",
    "\n",
    "    # Check the skew of all numerical features\n",
    "    skewed_feats = df[numeric_feats].apply(lambda x: skew(x)).sort_values(ascending=False)\n",
    "    high_skew = skewed_feats[abs(skewed_feats) > 0.5]\n",
    "    skewed_features = high_skew.index\n",
    "\n",
    "    for feat in skewed_features:\n",
    "        df[feat] = boxcox1p(df[feat], boxcox_normmax(df[feat] + 1))\n",
    "\n",
    "fixing_skewness(all_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = np.log1p(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = all_data[:len(train_df)]\n",
    "test_df = all_data[len(train_df):]\n",
    "\n",
    "X = train_df.copy()\n",
    "X_test = test_df.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Target Encoding the remaining object columns\n",
    "import category_encoders as ce\n",
    "\n",
    "target_encoder = ce.TargetEncoder(cols = object_col)\n",
    "\n",
    "target_encoder.fit(X[object_col], y)\n",
    "X[object_col] = target_encoder.transform(X[object_col], y)\n",
    "X_test[object_col] = target_encoder.transform(X_test[object_col])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalization\n",
    "\n",
    "from sklearn.preprocessing import RobustScaler\n",
    "\n",
    "robust_scaler = RobustScaler()\n",
    "X = robust_scaler.fit_transform(X)\n",
    "X_test = robust_scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = pd.DataFrame(X)\n",
    "X_test = pd.DataFrame(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ridge Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ridge model\n",
    "from sklearn.linear_model import Ridge\n",
    "\n",
    "ridge_model = Ridge(alpha= 0.01, normalize=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "scores = np.sqrt(-1*cross_val_score(ridge_model, X, y, cv=5, scoring='neg_mean_squared_error'))\n",
    "np.mean(scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ElasticNet Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import ElasticNet\n",
    "\n",
    "elastic_net_model = ElasticNet(alpha= 0.0001, normalize=True)\n",
    "\n",
    "scores2 = np.sqrt(-1*cross_val_score(elastic_net_model, X, y, cv=5, scoring='neg_mean_squared_error'))\n",
    "np.mean(scores2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lasso Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import Lasso\n",
    "\n",
    "lasso_model = Lasso(alpha=0.0001, normalize=True)\n",
    "\n",
    "scores3 = np.sqrt(-1*cross_val_score(lasso_model, X, y, cv=5, scoring='neg_mean_squared_error'))\n",
    "np.mean(scores3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LightGBM model with SelectKBest"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With X, y and y_test in hands, now we can make a model and train the data to make predictions.\n",
    "\n",
    "First, let's split the data into training and validation data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_val, y_train, y_val = train_test_split(X, y, \n",
    "                                                  train_size=0.8, \n",
    "                                                  test_size=0.2, \n",
    "                                                  random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# checking distribution of splitting\n",
    "print(y_train.mean())\n",
    "print(y_val.mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports\n",
    "from sklearn.feature_selection import SelectKBest, mutual_info_regression\n",
    "import lightgbm as lgb\n",
    "from sklearn.metrics import mean_squared_error"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define functions to select the best parameters and to evaluate score with mean absolute error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def select_k_best(X_train, y_train, X_val, k):\n",
    "    # select only 10 best features\n",
    "    selector = SelectKBest(mutual_info_regression, k=k)\n",
    "\n",
    "    # array w/o columns\n",
    "    X_k_train = selector.fit_transform(X_train, y_train)\n",
    "\n",
    "    # get dataframe with unused columns with zeros\n",
    "    selected_features = pd.DataFrame(selector.inverse_transform(X_k_train), \n",
    "                                     index=X_train.index, \n",
    "                                     columns = X_train.columns)\n",
    "\n",
    "    selected_columns = selected_features.columns[selected_features.var() != 0]\n",
    "\n",
    "    X_k_train = X_train[selected_columns]\n",
    "    X_k_val = X_val[selected_columns]\n",
    "\n",
    "    return (X_k_train, X_k_val)\n",
    "\n",
    "# LightGBM model\n",
    "def lgb_model(X_train, y_train, X_val, y_val):\n",
    "    \n",
    "    dtrain = lgb.Dataset(X_train, label=y_train)\n",
    "    dvalid = lgb.Dataset(X_val, label=y_val)\n",
    "\n",
    "    param = {'num_leaves':64,\n",
    "             'objective':'regression_l1', \n",
    "             'metric':'regression_l1', \n",
    "             'seed':7\n",
    "            }\n",
    "\n",
    "    bst = lgb.train(param, dtrain,\n",
    "                    num_boost_round = 1000, \n",
    "                    valid_sets=[dvalid],\n",
    "                    early_stopping_rounds=10,\n",
    "                    verbose_eval=False)\n",
    "    return(bst)\n",
    "\n",
    "# calculates score of model\n",
    "def score_rmse(X, y, bst):\n",
    "    pred = bst.predict(X)\n",
    "    score = np.sqrt(mean_squared_error(y, pred))\n",
    "    return(score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Find how many parameters give best results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dict_score = {}\n",
    "\n",
    "for k in tqdm.tqdm([10, 20, 30, 40, 50, 60, 70, 84]):\n",
    "    \n",
    "    X_k_train, X_k_val = select_k_best(X_train, y_train, X_val, k);\n",
    "    \n",
    "    model = lgb_model(X_k_train, y_train, X_k_val, y_val);\n",
    "    \n",
    "    dict_score[k] = score_rmse(X_k_val, y_val, model);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dict_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the models used ElasticNet model showed the best result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "elastic_net_model.fit(X, y)\n",
    "pred = elastic_net_model.predict(X_test)\n",
    "pred = np.expm1(pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create output to submit to Kaggle\n",
    "output = pd.DataFrame({'SalePrice': pred}, index=test_df.index)\n",
    "output.to_csv('/home/artur/titanic/data/external/elastic_net_model.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
